{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparison of two time frames on where water was then compared to where water is now\n",
    "This notebook is similar to the inundation mapping notebook. It links stream gauge data to satellite data and retrieves only the satellite images taken when the gauge was reading high-flow, allowing the user to study floods. This notebook allows the user to compare two time-frames for example, before a legislation was enforced and after it was enforced. The notebook generates a red and blue image showing the change in water occurance on floodplains and dams according to a certain date. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports and get data\n",
    "This notebook accesses some back-end Python and XML files that scrape gauge data from the BoM Water Data Online website. They are located in the folder called Scripts. For ease of use, we have cached all the data so the code will run even if the BoM website is down. If you want to access the live BoM website, you can go into Scripts and move the stations.pkl file out of there so the code can't find it, and then it will go look on the real website instead of in the cache.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading from cache\n",
      "4305 stations loaded; e.g.:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[namespace(name='15 MILE @ GRETA STH', pos=(-36.61945775, 146.24407214), url='http://bom.gov.au/waterdata/services/stations/403213'),\n",
       " namespace(name='15 MILE @ WANGARATTA', pos=(-36.36666667, 146.2833333), url='http://bom.gov.au/waterdata/services/stations/403239'),\n",
       " namespace(name='16 Mile Waterhole', pos=(-18.876921, 139.360487), url='http://bom.gov.au/waterdata/services/stations/913010A'),\n",
       " namespace(name='163 Clifton Rd', pos=(-32.97808, 115.90111), url='http://bom.gov.au/waterdata/services/stations/6131318'),\n",
       " namespace(name='18 Mile Swamp HorseX', pos=(-27.49561971, 153.50836409), url='http://bom.gov.au/waterdata/services/stations/144005A')]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import pickle\n",
    "import datacube\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "from IPython.display import display\n",
    "from datacube.storage import masking\n",
    "from datacube.utils import geometry \n",
    "from datacube.utils.geometry import CRS\n",
    "from datacube.helpers import write_geotiff\n",
    "\n",
    "sys.path.append('Scripts')\n",
    "import dea_bom\n",
    "\n",
    "dc = datacube.Datacube(app='Inundation_mapping')\n",
    "\n",
    "\n",
    "#retrieving data\n",
    "stations_pkl = Path('Scripts/stations.pkl')\n",
    "\n",
    "# If cache exists, get station data from cache\n",
    "if stations_pkl.exists():\n",
    "    print('Loading from cache')\n",
    "    stations = pickle.load(open(str(stations_pkl), 'rb'))\n",
    "    \n",
    "else:\n",
    "    print('Fetching from BoM')\n",
    "    stations = dea_bom.get_stations()\n",
    "    pickle.dump(stations, open(str(stations_pkl), 'wb'))\n",
    "\n",
    "# Filter list to stations with available data\n",
    "stations_with_data = pickle.load(open(str('Scripts/stations_with_data.pkl'), 'rb'))\n",
    "stations = [i for i in stations if i.name in stations_with_data]\n",
    "\n",
    "# Preview the first five stations loaded\n",
    "print(f'{len(stations)} stations loaded; e.g.:')\n",
    "stations[:5]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get the map\n",
    "This cell generates a map with the gauges on it. Select a gauge to get flow data from. Make sure there is data available both before and after the legislation started. Click 'Done' once you have found a station with good data. If you forget to click 'Done', the rest of the code won't run. If you want to choose another gauge after you have clicked 'Done', re-run this cell. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gauge_data, station = dea_bom.ui_select_station(stations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make the Flow Duration Curve, Dask load satellite data and merge with gauge data\n",
    "The lat and lon of the gauge will be automatically selected, but is changeable in the first line of code below. The top 20% of flows will automatically be selected. This is also changeable in the line of code below that says # What part of the Flow Duration Curve do you want to look at?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The lat and lon takes the location of the gauge. You can change the lat \n",
    "# and lon to a different location if necessary, just comment out out this \n",
    "# lat, lon = pos line below and define your own.\n",
    "lat, lon = station.pos\n",
    "\n",
    "#lat = -28.7495 \n",
    "#lon =  147.8791\n",
    "\n",
    "\n",
    "# The buffer is how many meters radius around the location you want to display.\n",
    "buffer = 20000\n",
    "\n",
    "# Rearranging data into a flow duration curve\n",
    "gauge_data = gauge_data.dropna()\n",
    "gauge_data = gauge_data.sort_values('Value')\n",
    "gauge_data['rownumber'] = np.arange(len(gauge_data))\n",
    "gauge_data['Exceedence'] = (1 - (gauge_data.rownumber / len(gauge_data))) * 100\n",
    "\n",
    "# What part of the Flow Duration Curve do you want to look at?\n",
    "xaxis_lower_parameter = 0\n",
    "xaxis_higher_parameter = 30\n",
    "\n",
    "# Plot the data on a log scale\n",
    "gauge_data.plot(x='Exceedence',\n",
    "                y='Value',\n",
    "                logy=True,\n",
    "                title='Selected range displayed on a log scale',\n",
    "                figsize=(10, 6))\n",
    "plt.axvspan(xaxis_lower_parameter,\n",
    "            xaxis_higher_parameter,\n",
    "            color='red',\n",
    "            alpha=0.2)\n",
    "plt.ylabel('Cubic meters per second (log)');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Query, Dask load and merge\n",
    "The output of this next box will tell you how many satellite passes you are about to load and the lat and lon of the area you are generating an image of. The query in this box should have the time set from 1987 to present, we want to load all the data here. The part where you choose the epochs comes later. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up a query which includes ALL satellite data from 1987\n",
    "x, y = geometry.point(lon, lat, CRS('WGS84')).to_crs(CRS('EPSG:3577')).points[0]\n",
    "query = {'x': (x - buffer, x + buffer),\n",
    "         'y': (y - buffer, y + buffer),    \n",
    "         'time': ('1987-01-01', '2019-11-01'), # Change this date to match present day\n",
    "         'crs': 'EPSG:3577'} \n",
    "\n",
    "# Dask load wofs_albers data (this loads the dataset parameters only, \n",
    "# not the actual satellite data)\n",
    "wofs_albers = dc.load(product = 'wofs_albers', \n",
    "                      dask_chunks = {}, \n",
    "                      group_by='solar_day', \n",
    "                      **query)\n",
    "\n",
    "\n",
    "# Merging satellite data with gauge data by timestamp\n",
    "gauge_data_xr = gauge_data.to_xarray()\n",
    "merged_data = gauge_data_xr.interp(Timestamp=wofs_albers.time, method='nearest')\n",
    "\n",
    "# Here is where it takes into account user input for the FDC\n",
    "specified_level = merged_data.where((merged_data.Exceedence > xaxis_lower_parameter) & \n",
    "                                    (merged_data.Exceedence < xaxis_higher_parameter), \n",
    "                                    drop=True)\n",
    "\n",
    "# Get list of dates to keep\n",
    "date_list = specified_level.time.values\n",
    "\n",
    "print(f'You are about to load {specified_level.time.shape[0]} satellite passes')\n",
    "\n",
    "print(f'lat = {lat}')\n",
    "print(f'lon = {lon}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate images of before and after\n",
    "Here's the part where you choose the epochs you are interested in comparing (where it says #Change according to date legislation became effective). This step will also load and cloud mask those satellite passes and generate 3 images: one for the 'before' epoch and one for the 'after' epoch ie according to the dates  in the query and a delta image, which shows the change in water frequency of 'before' minus 'after' on a blue to red scale. Blue means water occured after legislation and red means water occured before legislation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the passes that happened during the specified flow parameters\n",
    "specified_passes = wofs_albers.sel(time=date_list).compute()\n",
    "\n",
    "# Cloudmask\n",
    "cc = masking.make_mask(specified_passes.water, cloud=True)\n",
    "ncloud_pixels = cc.sum(dim=['x', 'y'])\n",
    "npixels_per_slice = (specified_passes.water.shape[1] * \n",
    "                     specified_passes.water.shape[2])\n",
    "cloud_pixels_fraction = (ncloud_pixels / npixels_per_slice)\n",
    "clear_specified_passes = specified_passes.water.isel(\n",
    "    time=cloud_pixels_fraction < 0.5)\n",
    "\n",
    "\n",
    "# Split by date into before and after legislation about floodplain harvesting\n",
    "query_dates = clear_specified_passes\n",
    "\n",
    "before_legislation = query_dates.loc[dict(time=slice('1987-01-01', '1998-06-01'))] # Change according to date legislation became effective\n",
    "after_legislation = query_dates.loc[dict(time=slice('1998-06-02', '2010-11-13'))]\n",
    "\n",
    "print(f'The Before image is made of {before_legislation.time.shape[0]} satellite passes')\n",
    "print(f'The After image is made of {after_legislation.time.shape[0]} satellite passes')\n",
    "\n",
    "#Create parameters for the image\n",
    "wet = (before_legislation == 128).sum(dim='time')\n",
    "dry = (before_legislation == 0).sum(dim='time')\n",
    "clear = wet + dry\n",
    "frequency_before = wet / clear\n",
    "frequency_before = frequency_before.fillna(0) #this is to get rid of the NAs that occur due to mountain shadows\n",
    "frequency_before = frequency_before.where(frequency_before !=0) #This is to tell it to make areas that were dry 100% of the time white\n",
    "\n",
    "#Plotting the image\n",
    "frequency_before.plot(figsize = (16, 12))\n",
    "plt.axis('off')\n",
    "plt.title('Before')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "#Create parameters for the image\n",
    "wet = (after_legislation == 128).sum(dim='time')\n",
    "dry = (after_legislation == 0).sum(dim='time')\n",
    "clear = wet + dry\n",
    "frequency_after = wet / clear\n",
    "frequency_after = frequency_after.fillna(0) #this is to get rid of the NAs that occur due to mountain shadows\n",
    "frequency_after = frequency_after.where(frequency_after !=0) #This is to tell it to make areas that were dry 100% of the time white\n",
    "\n",
    "#Plotting the image\n",
    "frequency_after.plot(figsize = (16, 12))\n",
    "plt.axis('off')\n",
    "plt.title('After')\n",
    "plt.show()\n",
    "\n",
    "delta = frequency_before - frequency_after\n",
    "\n",
    "# Plotting the image\n",
    "delta.plot(figsize = (16, 12), vmin = -0.2, vmax = 0.2, cmap = 'RdYlBu_r')\n",
    "plt.axis('off')\n",
    "plt.title(\"After minus before: Red means water is no longer there and blue means water has appeared there in the 'After' images\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check the distribution of the 'before' and 'after' passes\n",
    "To get a better understanding of the delta image, you might want to check the distribution of the 'before' and 'after' passes along the flow duration curve to get an idea if the results might be skewed. This part isn't automated to select the same dates yet, so please re-enter your before and after dates again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now take the clear passes and make a pandas dataframe that lists time of clear passes and corresponding gauge value\n",
    "clear_specified_passes_pd = clear_specified_passes.time.to_dataframe()\n",
    "clear_specified_passes_pd = clear_specified_passes_pd.rename(columns = {'time': 'date'})#can't have 2 columns called time\n",
    "merged_data_pd = merged_data.to_dataframe()\n",
    "\n",
    "#Merge clear satellite passes with gauge data by the time dimension\n",
    "clear_merged_data = pd.merge(clear_specified_passes_pd, merged_data_pd, left_on= 'time', \n",
    "                            right_index=True, how='inner')\n",
    "clear_merged_data = clear_merged_data.drop(columns='date')\n",
    "clear_merged_data = clear_merged_data.drop(columns='Timestamp')\n",
    "\n",
    "before = clear_merged_data['1987-01-10':'2008-06-01']\n",
    "after  = clear_merged_data['2008-06-01':]\n",
    "\n",
    "# Plot clear satellite passes with a top 20% flowrate over a flow duration curve\n",
    "ax = before.plot(x='Exceedence', marker = 'o', color = 'red', linestyle='None',\n",
    "                y='Value',\n",
    "                logy=True,\n",
    "                title='FDC log showing clear satellite passes of the before and after epochs', figsize = (20,15))\n",
    "after.plot(x='Exceedence', logy=True, y='Value', ax=ax, marker = 'o', color = 'blue', linestyle='None')\n",
    "gauge_data.plot(x='Exceedence', y='Value', logy=True, ax=ax, color = 'blue')\n",
    "ax.legend([\"before\", \"after\"]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the delta image as a GeoTIFF\n",
    "The ../ part of the file name means your .tif file will be saved one folder up from where this notebook is saved. Choose an appropriate file name. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the output file name \n",
    "file_name = '../delta_image.tif'\n",
    "\n",
    "# Set up the file for writing\n",
    "delta_image = delta.to_dataset()\n",
    "delta_image.attrs=wofs_albers.attrs\n",
    "\n",
    "# Write GeoTIFF to a location\n",
    "write_geotiff(file_name, delta_image)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
